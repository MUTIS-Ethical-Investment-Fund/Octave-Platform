{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a PoC that uses Reddit.\n",
        "\n",
        "Here is how I approached this:\n",
        "\n",
        " - For the Sentiment Analysis of the Macro-economic factors: I used the parameters given by Karun as keywords to scrape Reddit posts. This was in addition to the country_name given by the user and all of its aliases. So, now it returns a sentiment analysis of the factors in relation to the country. I used the best model I could find for the sentiment analysis.\n",
        "  - I did not use the further co-relations provided by Karun in the sentiment analysis process as using them there would be assuming the sentiment, and there would be no need to sentiment analyse. However, we could use them to associate the sentiment analysis with stock performance if we wish to do so.\n",
        "\n",
        "Note: The libraries installed and imported could be cleaned up as this includes some old models and trials. These are indicated.\n"
      ],
      "metadata": {
        "id": "z1SeIzksXyL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Everything"
      ],
      "metadata": {
        "id": "Ly1kUXcZX40P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install pandas\n",
        "!pip install pycountry\n",
        "!pip install numpy\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ULzrIpxXZh_",
        "outputId": "bae6ad4f-6d8e-4895-ba74-8afd77d674f4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.9/dist-packages (7.7.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.9/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.9/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.9/dist-packages (from praw) (1.5.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.9/dist-packages (22.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pycountry) (67.6.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import pycountry\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVaJGhTs2M2",
        "outputId": "f495898d-4b7b-4b17-f0a8-488f07302262"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Country Sentiment Analysis using Macro-Economic Factors\n",
        "### Sentiment Analysis with FinBert\n",
        "An attempt to use a better sentiment analyzer than NLTK Vader. FinBert- Tone: https://huggingface.co/yiyanghkust/finbert-tone.\n"
      ],
      "metadata": {
        "id": "E7r3vfcK5Rv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing it out"
      ],
      "metadata": {
        "id": "42Ir2GkVOHqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example text\n",
        "text = \"The Economy is amazing\"\n",
        "\n",
        "# Tokenize the text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Get the model outputs\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Get the predicted class scores\n",
        "scores = outputs.logits.softmax(dim=1).detach().numpy()[0]\n",
        "print(scores)\n",
        "\n",
        "# Get the label names\n",
        "label_names = model.config.id2label.values()\n",
        "\n",
        "# Save the scores to variables\n",
        "positive_score = scores[list(label_names).index('Positive')]\n",
        "negative_score = scores[list(label_names).index('Negative')]\n",
        "neutral_score = scores[list(label_names).index('Neutral')]\n",
        "\n",
        "print(positive_score)\n",
        "print(negative_score)\n",
        "print(neutral_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngAJpMBGUbSQ",
        "outputId": "2802636f-0179-4c78-9d3e-ac1bf7d832eb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.9124135e-08 9.9999988e-01 8.4741295e-08]\n",
            "0.9999999\n",
            "8.4741295e-08\n",
            "3.9124135e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementation on Reddit"
      ],
      "metadata": {
        "id": "MgARdd6eXpsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\t\n",
        "# Setting Up Data Cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    return text.lower()\n",
        "\n",
        "# Setting Up getting Country Aliases\n",
        "def get_country_aliases(country_name):\n",
        "    try:\n",
        "        country = pycountry.countries.search_fuzzy(country_name)[0]\n",
        "        aliases = [country.name]\n",
        "        if country.alpha_2 == 'US':\n",
        "            aliases += [country.alpha_2, country.alpha_3]\n",
        "        if hasattr(country, 'official_name'):\n",
        "            aliases.append(country.official_name)\n",
        "        if hasattr(country, 'common_name'):\n",
        "            aliases.append(country.common_name)\n",
        "        return aliases\n",
        "    except:\n",
        "        return []\n",
        "# Setting in Up Reddit Connection\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"aerXcM8ROdz47RuNMv2OGg\",\n",
        "    client_secret=\"dCoMUYzYC3NSHvL6kYH7z-1InRaAJg\",\n",
        "    user_agent=\"Sentimentent_Analysis_PoC/0.0.1\",\n",
        "    check_for_async=False, \n",
        ")\n",
        "\n",
        "# returns True if properly connected. This is a read_only instance.\n",
        "# print(reddit.read_only) \n",
        "\n",
        "# Setting up the Reddit Scraping\n",
        "country = input(\"Enter the name of the country: \")\n",
        "country_aliases = get_country_aliases(country)\n",
        "\n",
        "subreddits = [\"economics\", \"stocks\", \"investing\", \"wallstreetbets\"]\n",
        "keywords = [\"interest rate\", \"inflation\", \"exchange rate\", \"money supply\", \"GDP\", \"FII & FDI\", \"oil prices\", \"gold prices\"]\n",
        "ignored_keywords = [\"PSA: You can\"]  \n",
        "\n",
        "# Setting up the DataFrame to store the scraped posts\n",
        "df = pd.DataFrame(columns=['Title', 'Post', 'Subreddit', 'keywords_included', 'negative_score', 'neutral_score', 'positive_score', 'url'])\n",
        "\n",
        "# Actually Scraping and Sentiment Analyzing\n",
        "for sub in subreddits:\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    for keyword in keywords:\n",
        "        for post in subreddit.search(keyword, limit=10):\n",
        "            if any(ignored in post.title.lower() or ignored in post.selftext.lower() for ignored in ignored_keywords):\n",
        "                continue  # ignore the post if it contains ignored keywords\n",
        "            if post.score < 100:\n",
        "                continue  # ignore the post if it has less than 100 upvotes. This is an arbitrary number, and not researched at all.\n",
        "            post_text = post.title + \" \" + post.selftext\n",
        "            cleaned_text = clean_text(post_text)\n",
        "            if any(alias.lower() in cleaned_text for alias in country_aliases):\n",
        "                # Tokenize the text\n",
        "                inputs = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "                # Get the model outputs\n",
        "                outputs = model(**inputs)\n",
        "                # Get the predicted class scores\n",
        "                scores = outputs.logits.softmax(dim=1).detach().numpy()[0]\n",
        "                # Get the label names\n",
        "                label_names = model.config.id2label.values()\n",
        "                # Save the scores to variables\n",
        "                positive_score = scores[list(label_names).index('Positive')]\n",
        "                negative_score = scores[list(label_names).index('Negative')]\n",
        "                neutral_score = scores[list(label_names).index('Neutral')]\n",
        "                df = pd.concat([df, pd.DataFrame({\n",
        "                    'Title': [post.title],\n",
        "                    'Post': [post.selftext],\n",
        "                    'Subreddit': [sub],\n",
        "                    'keywords_included': [keyword],\n",
        "                    'negative_score': [negative_score],\n",
        "                    'neutral_score': [neutral_score],\n",
        "                    'positive_score': [positive_score],\n",
        "                    'url': [post.url]\n",
        "                })], ignore_index=True)\n",
        "\n",
        "# Add new columns to the dataframe\n",
        "# df['Highest Score'] = df[['negative_score', 'positive_score', 'neutral_score']].max(axis=1)\n",
        "df['Highest Score'] = df[['negative_score', 'positive_score', 'neutral_score']].idxmax(axis=1) + ': ' + df[['negative_score', 'positive_score', 'neutral_score']].max(axis=1).apply(lambda x: '{:.2f}'.format(x))\n",
        "\n",
        "# Calculate overall positive, negative, and neutral tone for each keyword\n",
        "df_agg = df.groupby('keywords_included').agg({'negative_score': 'mean', 'positive_score': 'mean', 'neutral_score': 'mean'})\n",
        "df_agg['Total'] = df_agg.sum(axis=1) # To assure that the aggregation process went well\n",
        "# This makes it a percentage if you are so inclined\n",
        "#df_agg[['negative_score', 'positive_score', 'neutral_score']] = df_agg[['negative_score', 'positive_score', 'neutral_score']].div(df_agg['Total'], axis=0).multiply(100)\n",
        "#df_agg = df_agg.drop(columns=['Total'])\n",
        "\n",
        "# Display the modified dataframe with posts, sentiments, and URLs\n",
        "display(df)\n",
        "\n",
        "# Display the aggregate sentiment score for each keyword\n",
        "display(df_agg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "G2guD8CsTJgi",
        "outputId": "26744394-443b-4f9c-bdb4-bcc7dc9dbdfe"
      },
      "execution_count": 91,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the name of the country: US\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 Title  \\\n",
              "0        Fed Interest Rate Decision Could Hurt Housing   \n",
              "1    Fed Tightening Reduces Horrendous Wealth Dispa...   \n",
              "2    Latest US inflation data raises questions abou...   \n",
              "3    Documents from 1 May to 30 August 2022 about t...   \n",
              "4    The Federal Reserve must choose between inflat...   \n",
              "..                                                 ...   \n",
              "161  1.1 million people are dead from covid-19. Wha...   \n",
              "162                                        Gold prices   \n",
              "163  Gold Prices Hit by Renewed Bets on Higher Yiel...   \n",
              "164  Is the gold price being manipulated to obfusca...   \n",
              "165  Why Gold Coin Demand Doesn't Drive the Gold Price   \n",
              "\n",
              "                                                  Post       Subreddit  \\\n",
              "0                                                            economics   \n",
              "1                                                            economics   \n",
              "2                                                            economics   \n",
              "3                                                            economics   \n",
              "4                                                            economics   \n",
              "..                                                 ...             ...   \n",
              "161  1.1 million people are dead from covid-19. Wha...  wallstreetbets   \n",
              "162  Ive been wondering about how Gold used to be u...  wallstreetbets   \n",
              "163  I was telling people not to buy Gold or Silver...  wallstreetbets   \n",
              "164                                                     wallstreetbets   \n",
              "165  Demand for newly fabricated coins makes up onl...  wallstreetbets   \n",
              "\n",
              "    keywords_included  negative_score  neutral_score  positive_score  \\\n",
              "0       interest rate        0.999997   6.862281e-07    2.264405e-06   \n",
              "1       interest rate        0.000309   9.907777e-01    8.913389e-03   \n",
              "2       interest rate        0.947478   5.241895e-02    1.027064e-04   \n",
              "3       interest rate        0.125963   8.709918e-01    3.044910e-03   \n",
              "4           inflation        0.000011   9.999886e-01    7.480334e-08   \n",
              "..                ...             ...            ...             ...   \n",
              "161       gold prices        0.975084   2.198211e-02    2.933634e-03   \n",
              "162       gold prices        0.019780   9.799078e-01    3.121550e-04   \n",
              "163       gold prices        0.004245   9.561986e-01    3.955654e-02   \n",
              "164       gold prices        0.033720   9.637516e-01    2.528664e-03   \n",
              "165       gold prices        0.007112   9.906014e-01    2.286601e-03   \n",
              "\n",
              "                                                   url         Highest Score  \n",
              "0    https://cepr.net/fed-interest-rate-decision-co...  negative_score: 1.00  \n",
              "1    https://wolfstreet.com/2022/12/21/fed-tighteni...   neutral_score: 0.99  \n",
              "2    https://www.theguardian.com/business/2022/oct/...  negative_score: 0.95  \n",
              "3    https://www.rba.gov.au/information/foi/disclos...   neutral_score: 0.87  \n",
              "4    https://finance.yahoo.com/news/federal-must-ch...   neutral_score: 1.00  \n",
              "..                                                 ...                   ...  \n",
              "161  https://www.reddit.com/r/wallstreetbets/commen...  negative_score: 0.98  \n",
              "162  https://www.reddit.com/r/wallstreetbets/commen...   neutral_score: 0.98  \n",
              "163  https://www.reddit.com/r/wallstreetbets/commen...   neutral_score: 0.96  \n",
              "164              https://www.reddit.com/gallery/y4mumr   neutral_score: 0.96  \n",
              "165  https://www.reddit.com/r/wallstreetbets/commen...   neutral_score: 0.99  \n",
              "\n",
              "[166 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4f12b50-c315-43fe-a5d4-ff0d594ababb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Post</th>\n",
              "      <th>Subreddit</th>\n",
              "      <th>keywords_included</th>\n",
              "      <th>negative_score</th>\n",
              "      <th>neutral_score</th>\n",
              "      <th>positive_score</th>\n",
              "      <th>url</th>\n",
              "      <th>Highest Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fed Interest Rate Decision Could Hurt Housing</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>0.999997</td>\n",
              "      <td>6.862281e-07</td>\n",
              "      <td>2.264405e-06</td>\n",
              "      <td>https://cepr.net/fed-interest-rate-decision-co...</td>\n",
              "      <td>negative_score: 1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fed Tightening Reduces Horrendous Wealth Dispa...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>9.907777e-01</td>\n",
              "      <td>8.913389e-03</td>\n",
              "      <td>https://wolfstreet.com/2022/12/21/fed-tighteni...</td>\n",
              "      <td>neutral_score: 0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Latest US inflation data raises questions abou...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>0.947478</td>\n",
              "      <td>5.241895e-02</td>\n",
              "      <td>1.027064e-04</td>\n",
              "      <td>https://www.theguardian.com/business/2022/oct/...</td>\n",
              "      <td>negative_score: 0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Documents from 1 May to 30 August 2022 about t...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>0.125963</td>\n",
              "      <td>8.709918e-01</td>\n",
              "      <td>3.044910e-03</td>\n",
              "      <td>https://www.rba.gov.au/information/foi/disclos...</td>\n",
              "      <td>neutral_score: 0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Federal Reserve must choose between inflat...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>inflation</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>9.999886e-01</td>\n",
              "      <td>7.480334e-08</td>\n",
              "      <td>https://finance.yahoo.com/news/federal-must-ch...</td>\n",
              "      <td>neutral_score: 1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>1.1 million people are dead from covid-19. Wha...</td>\n",
              "      <td>1.1 million people are dead from covid-19. Wha...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>0.975084</td>\n",
              "      <td>2.198211e-02</td>\n",
              "      <td>2.933634e-03</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>negative_score: 0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>Gold prices</td>\n",
              "      <td>Ive been wondering about how Gold used to be u...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>0.019780</td>\n",
              "      <td>9.799078e-01</td>\n",
              "      <td>3.121550e-04</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>neutral_score: 0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Gold Prices Hit by Renewed Bets on Higher Yiel...</td>\n",
              "      <td>I was telling people not to buy Gold or Silver...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>0.004245</td>\n",
              "      <td>9.561986e-01</td>\n",
              "      <td>3.955654e-02</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>neutral_score: 0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Is the gold price being manipulated to obfusca...</td>\n",
              "      <td></td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>0.033720</td>\n",
              "      <td>9.637516e-01</td>\n",
              "      <td>2.528664e-03</td>\n",
              "      <td>https://www.reddit.com/gallery/y4mumr</td>\n",
              "      <td>neutral_score: 0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>Why Gold Coin Demand Doesn't Drive the Gold Price</td>\n",
              "      <td>Demand for newly fabricated coins makes up onl...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>0.007112</td>\n",
              "      <td>9.906014e-01</td>\n",
              "      <td>2.286601e-03</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>neutral_score: 0.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4f12b50-c315-43fe-a5d4-ff0d594ababb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4f12b50-c315-43fe-a5d4-ff0d594ababb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4f12b50-c315-43fe-a5d4-ff0d594ababb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                   negative_score  positive_score  neutral_score  Total\n",
              "keywords_included                                                      \n",
              "FII & FDI                0.004496        0.031080       0.964424    1.0\n",
              "GDP                      0.236769        0.217824       0.545407    1.0\n",
              "exchange rate            0.085669        0.122229       0.792102    1.0\n",
              "gold prices              0.168358        0.248516       0.583126    1.0\n",
              "inflation                0.172598        0.143636       0.683767    1.0\n",
              "interest rate            0.139399        0.129778       0.730823    1.0\n",
              "money supply             0.146760        0.204275       0.648966    1.0\n",
              "oil prices               0.320013        0.184387       0.495600    1.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4fb9bb46-4a72-4a1c-a2ad-77ca0cde068c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative_score</th>\n",
              "      <th>positive_score</th>\n",
              "      <th>neutral_score</th>\n",
              "      <th>Total</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>keywords_included</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>FII &amp; FDI</th>\n",
              "      <td>0.004496</td>\n",
              "      <td>0.031080</td>\n",
              "      <td>0.964424</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GDP</th>\n",
              "      <td>0.236769</td>\n",
              "      <td>0.217824</td>\n",
              "      <td>0.545407</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exchange rate</th>\n",
              "      <td>0.085669</td>\n",
              "      <td>0.122229</td>\n",
              "      <td>0.792102</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gold prices</th>\n",
              "      <td>0.168358</td>\n",
              "      <td>0.248516</td>\n",
              "      <td>0.583126</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>inflation</th>\n",
              "      <td>0.172598</td>\n",
              "      <td>0.143636</td>\n",
              "      <td>0.683767</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>interest rate</th>\n",
              "      <td>0.139399</td>\n",
              "      <td>0.129778</td>\n",
              "      <td>0.730823</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>money supply</th>\n",
              "      <td>0.146760</td>\n",
              "      <td>0.204275</td>\n",
              "      <td>0.648966</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oil prices</th>\n",
              "      <td>0.320013</td>\n",
              "      <td>0.184387</td>\n",
              "      <td>0.495600</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4fb9bb46-4a72-4a1c-a2ad-77ca0cde068c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4fb9bb46-4a72-4a1c-a2ad-77ca0cde068c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4fb9bb46-4a72-4a1c-a2ad-77ca0cde068c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying to implement it using Inference API \n",
        "Limits didn't allow me to test it"
      ],
      "metadata": {
        "id": "xKa1n1nQTKUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing and Playing with the FinBert - Tone Model using the Inference API"
      ],
      "metadata": {
        "id": "TJWcKIciOAcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/yiyanghkust/finbert-tone\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "scores = query({\"inputs\": \"The economy is going up\"})\n",
        "print(scores)\n",
        "\n",
        "negative_score = scores[0][0]['score']\n",
        "neutral_score = scores[0][1]['score']\n",
        "positive_score = scores[0][2]['score']\n",
        "\n",
        "print(negative_score)\n",
        "print(neutral_score)\n",
        "print(positive_score)\n",
        "\n",
        "def get_scores(output):\n",
        "    scores = {}\n",
        "    for item in output[0]:\n",
        "        scores[item['label']] = item['score']\n",
        "    return scores.get('Negative', 0), scores.get('Neutral', 0), scores.get('Positive', 0)\n",
        "\n",
        "get_scores(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8awRwtnMpUb",
        "outputId": "a807e17f-0970-44d4-c14c-f97f12d8b84a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'label': 'Neutral', 'score': 0.7878367900848389}, {'label': 'Negative', 'score': 0.15684927999973297}, {'label': 'Positive', 'score': 0.05531390383839607}]]\n",
            "0.7878367900848389\n",
            "0.15684927999973297\n",
            "0.05531390383839607\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.15684927999973297, 0.7878367900848389, 0.05531390383839607)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing it \n",
        "Failed attempt due to inference API limits."
      ],
      "metadata": {
        "id": "LJfIi8uIXhLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/yiyanghkust/finbert-tone\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\t\n",
        "\n",
        "# Setting Up Data Cleaning\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    return text.lower()\n",
        "\n",
        "# Setting Up getting Country Aliases\n",
        "def get_country_aliases(country_name):\n",
        "    try:\n",
        "        country = pycountry.countries.search_fuzzy(country_name)[0]\n",
        "        aliases = [country.name]\n",
        "        if country.alpha_2 == 'US':\n",
        "            aliases += [country.alpha_2, country.alpha_3]\n",
        "        if hasattr(country, 'official_name'):\n",
        "            aliases.append(country.official_name)\n",
        "        if hasattr(country, 'common_name'):\n",
        "            aliases.append(country.common_name)\n",
        "        return aliases\n",
        "    except:\n",
        "        return []\n",
        "# Setting in Up Reddit Connection\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"aerXcM8ROdz47RuNMv2OGg\",\n",
        "    client_secret=\"dCoMUYzYC3NSHvL6kYH7z-1InRaAJg\",\n",
        "    user_agent=\"Sentimentent_Analysis_PoC/0.0.1\",\n",
        "    check_for_async=False, \n",
        ")\n",
        "\n",
        "# returns True if properly connected. This is a read_only instance.\n",
        "# print(reddit.read_only) \n",
        "\n",
        "# Setting up the Reddit Scraping\n",
        "country = input(\"Enter the name of the country: \")\n",
        "country_aliases = get_country_aliases(country)\n",
        "\n",
        "subreddits = [\"economics\", \"stocks\", \"investing\", \"wallstreetbets\"]\n",
        "keywords = [\"interest rate\", \"inflation\", \"exchange rate\", \"money supply\", \"GDP\", \"FII & FDI\", \"oil prices\", \"gold prices\"]\n",
        "ignored_keywords = [\"PSA: You can\"]  \n",
        "\n",
        "# Setting up Function to order Model Output\n",
        "def get_scores(output):\n",
        "    scores = {}\n",
        "    for item in output[0]:\n",
        "        scores[item['label']] = item['score']\n",
        "    return scores.get('Negative', 0), scores.get('Neutral', 0), scores.get('Positive', 0)\n",
        "\n",
        "# Setting up the DataFrame to store the scraped posts\n",
        "df = pd.DataFrame(columns=['Title', 'Post', 'Subreddit', 'keywords_included', 'negative_score', 'neutral_score', 'positive_score', 'url'])\n",
        "\n",
        "# Actually Scraping and Sentiment Analyzing\n",
        "for sub in subreddits:\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    for keyword in keywords:\n",
        "        for post in subreddit.search(keyword, limit=10):\n",
        "            if any(ignored in post.title.lower() or ignored in post.selftext.lower() for ignored in ignored_keywords):\n",
        "                continue  # ignore the post if it contains ignored keywords\n",
        "            post_text = post.title + \" \" + post.selftext\n",
        "            cleaned_text = clean_text(post_text)\n",
        "            if any(alias.lower() in cleaned_text for alias in country_aliases):\n",
        "                sentiment_scores = query({'inputs': cleaned_text})\n",
        "                sentiment_scores = get_scores(sentiment_scores)\n",
        "                negative_score = sentiment_scores[0]\n",
        "                neutral_score = sentiment_scores[1]\n",
        "                positive_score = sentiment_scores[2]\n",
        "                df = pd.concat([df, pd.DataFrame({\n",
        "                    'Title': [post.title],\n",
        "                    'Post': [post.selftext],\n",
        "                    'Subreddit': [sub],\n",
        "                    'keywords_included': [keyword],\n",
        "                    'negative_score': [negative_score],\n",
        "                    'neutral_score': [neutral_score],\n",
        "                    'positive_score': [positive_score],\n",
        "                    'url': [post.url]\n",
        "                })], ignore_index=True)\n",
        "\n",
        "# Add new columns to the dataframe\n",
        "df['Highest Score'] = df[['Negative', 'Positive', 'Neutral']].max(axis=1)\n",
        "\n",
        "# Calculate percentage of positive, negative, and neutral tone for each keyword\n",
        "df_agg = df.groupby('keywords_included').agg({'Negative': 'mean', 'Positive': 'mean', 'Neutral': 'mean'})\n",
        "df_agg['Total'] = df_agg.sum(axis=1)\n",
        "df_agg[['Negative', 'Positive', 'Neutral']] = df_agg[['Negative', 'Positive', 'Neutral']].div(df_agg['Total'], axis=0).multiply(100)\n",
        "df_agg = df_agg.drop(columns=['Total'])\n",
        "\n",
        "# Display the modified dataframe with posts, sentiments, and URLs\n",
        "display(df)\n",
        "\n",
        "# Display the aggregate sentiment score for each keyword\n",
        "display(df_agg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "u0tH7TnYLApJ",
        "outputId": "c4e9dc0d-10d4-44fc-deae-166cab4ccdec"
      },
      "execution_count": 74,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the name of the country: US\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-a0709ee8ab08>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_text\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malias\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcountry_aliases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0msentiment_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0msentiment_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0mnegative_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mneutral_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-a0709ee8ab08>\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Negative'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Neutral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uses NLTK Vader\n",
        "which takes context into account. From a cursory glance at its performance, it doesn't seem to do well. Thought its performance should be better than textblob.\n",
        "\n",
        "https://insight-group.github.io/MFIN7036/sentiment-analysis-lexicon-based-nv-or-tb.html#:~:text=NLTK%20Vader%20focus%20on%20analyzing,entities%20into%20consideration%20by%20POS.\n"
      ],
      "metadata": {
        "id": "_gpTwW0KBZ9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Up Data Cleaning\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    return text.lower()\n",
        "\n",
        "# Setting Up getting Country Aliases\n",
        "def get_country_aliases(country_name):\n",
        "    try:\n",
        "        country = pycountry.countries.search_fuzzy(country_name)[0]\n",
        "        aliases = [country.name]\n",
        "        if country.alpha_2 == 'US':\n",
        "            aliases += [country.alpha_2, country.alpha_3]\n",
        "        if hasattr(country, 'official_name'):\n",
        "            aliases.append(country.official_name)\n",
        "        if hasattr(country, 'common_name'):\n",
        "            aliases.append(country.common_name)\n",
        "        return aliases\n",
        "    except:\n",
        "        return []\n",
        "# Setting in Up Reddit Connection\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"aerXcM8ROdz47RuNMv2OGg\",\n",
        "    client_secret=\"dCoMUYzYC3NSHvL6kYH7z-1InRaAJg\",\n",
        "    user_agent=\"Sentimentent_Analysis_PoC/0.0.1\",\n",
        "    check_for_async=False, \n",
        ")\n",
        "\n",
        "# returns True if properly connected. This is a read_only instance.\n",
        "# print(reddit.read_only) \n",
        "\n",
        "# Setting up the Reddit Scraping\n",
        "country = input(\"Enter the name of the country: \")\n",
        "country_aliases = get_country_aliases(country)\n",
        "\n",
        "subreddits = [\"economics\", \"stocks\", \"investing\", \"wallstreetbets\"]\n",
        "keywords = [\"interest rate\", \"inflation\", \"exchange rate\", \"money supply\", \"GDP\", \"FII & FDI\", \"oil prices\", \"gold prices\"]\n",
        "ignored_keywords = [\"PSA: You can\"]  \n",
        "# Setting up the DataFrame to store the scraped posts\n",
        "df = pd.DataFrame(columns=['Title', 'Post', 'Subreddit', 'keywords_included', 'sentiment_score', 'url'])\n",
        "\n",
        "for sub in subreddits:\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    for keyword in keywords:\n",
        "        for post in subreddit.search(keyword, limit=10):\n",
        "            if any(ignored in post.title.lower() or ignored in post.selftext.lower() for ignored in ignored_keywords):\n",
        "                continue  # ignore the post if it contains ignored keywords\n",
        "            post_text = post.title + \" \" + post.selftext\n",
        "            cleaned_text = clean_text(post_text)\n",
        "            if any(alias.lower() in cleaned_text for alias in country_aliases):\n",
        "                sentiment_score = sia.polarity_scores(cleaned_text)['compound']\n",
        "                df = pd.concat([df, pd.DataFrame({\n",
        "                    'Title': [post.title],\n",
        "                    'Post': [post.selftext],\n",
        "                    'Subreddit': [sub],\n",
        "                    'keywords_included': [keyword],\n",
        "                    'sentiment_score': [sentiment_score],\n",
        "                    'url': [post.url]\n",
        "                })], ignore_index=True)\n",
        "# Calculate aggregate sentiment score for each keyword\n",
        "aggregate_sentiment_scores = df.groupby(['keywords_included'])['sentiment_score'].mean()\n",
        "\n",
        "# Display the dataframe with posts, sentiments, and URLs\n",
        "display(df)\n",
        "\n",
        "# Display the aggregate sentiment score for each keyword\n",
        "display(aggregate_sentiment_scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "p5794OPQT4ND",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "outputId": "977e6729-86c7-4df8-85e6-931d38831430"
      },
      "execution_count": 54,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the name of the country: US\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 Title  \\\n",
              "0        Fed Interest Rate Decision Could Hurt Housing   \n",
              "1    Fed Tightening Reduces Horrendous Wealth Dispa...   \n",
              "2    Latest US inflation data raises questions abou...   \n",
              "3    Documents from 1 May to 30 August 2022 about t...   \n",
              "4    The Federal Reserve must choose between inflat...   \n",
              "..                                                 ...   \n",
              "162  1.1 million people are dead from covid-19. Wha...   \n",
              "163                                        Gold prices   \n",
              "164  Gold Prices Hit by Renewed Bets on Higher Yiel...   \n",
              "165  Is the gold price being manipulated to obfusca...   \n",
              "166  Why Gold Coin Demand Doesn't Drive the Gold Price   \n",
              "\n",
              "                                                  Post       Subreddit  \\\n",
              "0                                                            economics   \n",
              "1                                                            economics   \n",
              "2                                                            economics   \n",
              "3                                                            economics   \n",
              "4                                                            economics   \n",
              "..                                                 ...             ...   \n",
              "162  1.1 million people are dead from covid-19. Wha...  wallstreetbets   \n",
              "163  Ive been wondering about how Gold used to be u...  wallstreetbets   \n",
              "164  I was telling people not to buy Gold or Silver...  wallstreetbets   \n",
              "165                                                     wallstreetbets   \n",
              "166  Demand for newly fabricated coins makes up onl...  wallstreetbets   \n",
              "\n",
              "    keywords_included  sentiment_score  \\\n",
              "0       interest rate          -0.1027   \n",
              "1       interest rate          -0.0516   \n",
              "2       interest rate           0.4588   \n",
              "3       interest rate           0.8176   \n",
              "4           inflation          -0.5719   \n",
              "..                ...              ...   \n",
              "162       gold prices          -0.9957   \n",
              "163       gold prices          -0.7845   \n",
              "164       gold prices           0.9538   \n",
              "165       gold prices          -0.3818   \n",
              "166       gold prices           0.8574   \n",
              "\n",
              "                                                   url  \n",
              "0    https://cepr.net/fed-interest-rate-decision-co...  \n",
              "1    https://wolfstreet.com/2022/12/21/fed-tighteni...  \n",
              "2    https://www.theguardian.com/business/2022/oct/...  \n",
              "3    https://www.rba.gov.au/information/foi/disclos...  \n",
              "4    https://finance.yahoo.com/news/federal-must-ch...  \n",
              "..                                                 ...  \n",
              "162  https://www.reddit.com/r/wallstreetbets/commen...  \n",
              "163  https://www.reddit.com/r/wallstreetbets/commen...  \n",
              "164  https://www.reddit.com/r/wallstreetbets/commen...  \n",
              "165              https://www.reddit.com/gallery/y4mumr  \n",
              "166  https://www.reddit.com/r/wallstreetbets/commen...  \n",
              "\n",
              "[167 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7112cfb7-7c70-4ccb-aeb1-44d05a78a94b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Post</th>\n",
              "      <th>Subreddit</th>\n",
              "      <th>keywords_included</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fed Interest Rate Decision Could Hurt Housing</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>-0.1027</td>\n",
              "      <td>https://cepr.net/fed-interest-rate-decision-co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fed Tightening Reduces Horrendous Wealth Dispa...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>-0.0516</td>\n",
              "      <td>https://wolfstreet.com/2022/12/21/fed-tighteni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Latest US inflation data raises questions abou...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>0.4588</td>\n",
              "      <td>https://www.theguardian.com/business/2022/oct/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Documents from 1 May to 30 August 2022 about t...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>interest rate</td>\n",
              "      <td>0.8176</td>\n",
              "      <td>https://www.rba.gov.au/information/foi/disclos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Federal Reserve must choose between inflat...</td>\n",
              "      <td></td>\n",
              "      <td>economics</td>\n",
              "      <td>inflation</td>\n",
              "      <td>-0.5719</td>\n",
              "      <td>https://finance.yahoo.com/news/federal-must-ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>1.1 million people are dead from covid-19. Wha...</td>\n",
              "      <td>1.1 million people are dead from covid-19. Wha...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>-0.9957</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Gold prices</td>\n",
              "      <td>Ive been wondering about how Gold used to be u...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>-0.7845</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Gold Prices Hit by Renewed Bets on Higher Yiel...</td>\n",
              "      <td>I was telling people not to buy Gold or Silver...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>0.9538</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>Is the gold price being manipulated to obfusca...</td>\n",
              "      <td></td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>-0.3818</td>\n",
              "      <td>https://www.reddit.com/gallery/y4mumr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>Why Gold Coin Demand Doesn't Drive the Gold Price</td>\n",
              "      <td>Demand for newly fabricated coins makes up onl...</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>gold prices</td>\n",
              "      <td>0.8574</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>167 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7112cfb7-7c70-4ccb-aeb1-44d05a78a94b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7112cfb7-7c70-4ccb-aeb1-44d05a78a94b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7112cfb7-7c70-4ccb-aeb1-44d05a78a94b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "keywords_included\n",
              "FII & FDI       -0.011625\n",
              "GDP              0.079013\n",
              "exchange rate    0.236321\n",
              "gold prices      0.184271\n",
              "inflation        0.486215\n",
              "interest rate    0.598964\n",
              "money supply     0.226635\n",
              "oil prices      -0.025983\n",
              "Name: sentiment_score, dtype: float64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old Implementations\n",
        "Honestly not sure why I haven't deleted them"
      ],
      "metadata": {
        "id": "LJuisFkyMXHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old Scrape Implementation"
      ],
      "metadata": {
        "id": "J6V4aDsF9dAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for sub in subreddits:\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    for post in subreddit.hot(limit=100):\n",
        "        post_text = post.title + \" \" + post.selftext\n",
        "        post_text = clean_text(post_text)\n",
        "        for keyword in keywords:\n",
        "            if keyword.lower() in post_text and any(alias.lower() in post_text for alias in country_aliases):\n",
        "                sentiment_score = sia.polarity_scores(post_text)['compound']\n",
        "                df = pd.concat([df, pd.DataFrame({'Title': [post.title], 'Post': [post.selftext], 'Subreddit': [sub], 'keywords_included': [keyword], 'sentiment_score': [sentiment_score], 'url': [post.url]})], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "wbTGJlrB6Y-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ESG Sentiment Analysis\n",
        "This is still a work in progress. We have two options:\n",
        "1. I directly use the data provided by Anaya, but there is the issue of deciding on the keywords we will use for every theme. This will require someone else to give me a keyword list for each parameter to implement. I think this will take a lot of time and effort, and may not be worth it.\n",
        "2. I found these two models from the same academic that I got the sentiment analysis model from.\n",
        "- https://huggingface.co/yiyanghkust/finbert-esg-9-categories?text=For+2002%2C+our+total+net+emissions+were+approximately+60+million+metric+tons+of+CO2+equivalents+for+all+businesses+and+operations+we+have+%EF%AC%81nancial+interests+in%2C+based+on+its+equity+share+in+those+businesses+and+operations.+This+is+more+than+our+competition\n",
        "- https://huggingface.co/yiyanghkust/finbert-esg\n",
        "\n",
        "You give them text, and one roughly gives you whether it belongs in E or S or G or None, and the other is divided into 9 Categories.\n",
        "\n",
        "\n",
        "I am not actually sure how we will use those models for the final ESG Sentiment Analysis.\n",
        "\n",
        " But, my first thoughts , if we want to do ESG Sentiment Analysis on all platforms, starting from Reddit, is that we could try cycling through Reddit Posts with the company name, and get a certain number for each category of the 9 or perhaps just a certain number of E or S or G. Then perform Sentiment Analysis on them and get an overall sentiment score for each category (the 3 or the 9) and compare it with the overall sentiment score for each category in the sector. To do this, I would need keywords to identify the sector to do the same process that I listed just previously.\n",
        "\n",
        " I think a better use of our time would be to see if we do this on the Annual Report of the company in question, and compare it to the overall sentiment score in each category of its comparables (which would need to be inputted by tehh user)."
      ],
      "metadata": {
        "id": "EEMep64diPEI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-CqaMZymJKQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}